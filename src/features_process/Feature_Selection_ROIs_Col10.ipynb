{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41bd4272",
   "metadata": {
    "id": "41bd4272"
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fvSYfDDhGr_G",
   "metadata": {
    "id": "fvSYfDDhGr_G"
   },
   "source": [
    "### imports packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1bd0253",
   "metadata": {
    "id": "e1bd0253"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from numpy import set_printoptions\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from sklearn import manifold, datasets\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d435efa",
   "metadata": {
    "id": "2d435efa"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SAMR3bGOjdCj",
   "metadata": {
    "id": "SAMR3bGOjdCj"
   },
   "source": [
    "### loading Data from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "uHoK4uvAjwHS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHoK4uvAjwHS",
    "outputId": "250437e1-6ffd-42b1-e4a8-0d2e3edda425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 >> /home/superuser/Dados/mapbiomas/dadosCol10/ROIsv2/ROIsv2/7619.csv\n",
      "#3 >> /home/superuser/Dados/mapbiomas/dadosCol10/ROIsv2/ROIsv2/7712.csv\n",
      "#4 >> /home/superuser/Dados/mapbiomas/dadosCol10/ROIsv2/ROIsv2/765.csv\n",
      "#6 >> /home/superuser/Dados/mapbiomas/dadosCol10/ROIsv2/ROIsv2/7746.csv\n",
      "#10 >> /home/superuser/Dados/mapbiomas/dadosCol10/ROIsv2/ROIsv2/7615.csv\n",
      "#11 >> /home/superuser/Dados/mapbiomas/dadosCol10/ROIsv2/ROIsv2/773.csv\n"
     ]
    }
   ],
   "source": [
    "path_base = '/home/superuser/Dados/mapbiomas/dadosCol10/ROIsv2/ROIsv2'\n",
    "pathFeaturesBase = '/home/superuser/Dados/mapbiomas/dadosCol10/fileFeatSelect'\n",
    "lstfiles = glob.glob(path_base + '/*')\n",
    "# print(lstfiles)\n",
    "lstpathfiles = []\n",
    "for cc, npath in enumerate(lstfiles):\n",
    "    if 'rois_grade' not in npath:\n",
    "        print(f\"#{cc} >> {npath}\")\n",
    "        lstpathfiles.append(npath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wyvBo8CuHAh8",
   "metadata": {
    "id": "wyvBo8CuHAh8"
   },
   "source": [
    "### Load Tables and paramenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "v8Pq2ueXr4tz",
   "metadata": {
    "id": "v8Pq2ueXr4tz"
   },
   "outputs": [],
   "source": [
    "# read many files CSVs\n",
    "manyfile = False\n",
    "filterYear = True\n",
    "nyear = 2023\n",
    "# lstpathfiles = glob.glob(os.path.join(path_base, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "diyoNFd9fiNP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "diyoNFd9fiNP",
    "outputId": "21c7d78e-0a45-44ba-8565-71b021c053b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "dftmp = pd.read_csv(lstpathfiles[4])\n",
    "dftmp = dftmp.drop(['GRID_ID','system:index','.geo'], axis=1)\n",
    "lstCol = list(dftmp.columns)\n",
    "print(len(lstCol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "Vs6UUNurhu7M",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vs6UUNurhu7M",
    "outputId": "0826fd5b-1976-4ef0-a3a2-f4da035eaa1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"afvi_median\",\"afvi_median_dry\",\"afvi_median_wet\",\"avi_median\",\"avi_median_dry\",\"avi_median_wet\",\n",
      "\"awei_median\",\"awei_median_dry\",\"awei_median_wet\",\"blue_median\",\"blue_median_dry\",\n",
      "\"blue_median_wet\",\"blue_stdDev\",\"brba_median\",\"brba_median_dry\",\"brba_median_wet\",\n",
      "\"brightness_median\",\"brightness_median_dry\",\"brightness_median_wet\",\"bsi_median\",\"bsi_median_1\",\n",
      "\"bsi_median_2\",\"class\",\"cvi_median\",\"cvi_median_dry\",\"cvi_median_wet\",\n",
      "\"dswi5_median\",\"dswi5_median_dry\",\"dswi5_median_wet\",\"evi_median\",\"evi_median_dry\",\n",
      "\"evi_median_wet\",\"gcvi_median\",\"gcvi_median_dry\",\"gcvi_median_wet\",\"gemi_median\",\n",
      "\"gemi_median_dry\",\"gemi_median_wet\",\"gli_median\",\"gli_median_dry\",\"gli_median_wet\",\n",
      "\"green_median\",\"green_median_dry\",\"green_median_wet\",\"green_stdDev\",\"gvmi_median\",\n",
      "\"gvmi_median_dry\",\"gvmi_median_wet\",\"hillshade\",\"iia_median\",\"iia_median_dry\",\n",
      "\"iia_median_wet\",\"lswi_median\",\"lswi_median_dry\",\"lswi_median_wet\",\"mbi_median\",\n",
      "\"mbi_median_dry\",\"mbi_median_wet\",\"nddi_median\",\"nddi_median_dry\",\"nddi_median_wet\",\n",
      "\"ndvi_median\",\"ndvi_median_dry\",\"ndvi_median_wet\",\"ndwi_median\",\"ndwi_median_dry\",\n",
      "\"ndwi_median_wet\",\"nir_median\",\"nir_median_contrast\",\"nir_median_dry\",\"nir_median_dry_contrast\",\n",
      "\"nir_median_wet\",\"nir_stdDev\",\"osavi_median\",\"osavi_median_dry\",\"osavi_median_wet\",\n",
      "\"ratio_median\",\"ratio_median_dry\",\"ratio_median_wet\",\"red_median\",\"red_median_contrast\",\n",
      "\"red_median_dry\",\"red_median_dry_contrast\",\"red_median_wet\",\"red_stdDev\",\"ri_median\",\n",
      "\"ri_median_dry\",\"ri_median_wet\",\"rvi_median\",\"rvi_median_1\",\"rvi_median_wet\",\n",
      "\"shape_median\",\"shape_median_dry\",\"shape_median_wet\",\"solpe\",\"swir1_median\",\n",
      "\"swir1_median_dry\",\"swir1_median_wet\",\"swir1_stdDev\",\"swir2_median\",\"swir2_median_dry\",\n",
      "\"swir2_median_wet\",\"swir2_stdDev\",\"ui_median\",\"ui_median_dry\",\"ui_median_wet\",\n",
      "\"wetness_median\",\"wetness_median_dry\",\"wetness_median_wet\",\"year\",\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "for ii in range(0, len(lstCol)):\n",
    "    text = text + f'\"{lstCol[ii]}\",'\n",
    "    if ii % 5 == 0 and ii > 0:\n",
    "        print(text)\n",
    "        text = ''\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1HeNYU0t36Mi",
   "metadata": {
    "id": "1HeNYU0t36Mi"
   },
   "source": [
    "### Analises de Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eGLUTvyx1ooX",
   "metadata": {
    "id": "eGLUTvyx1ooX"
   },
   "outputs": [],
   "source": [
    "class processin_features_byYears(object):\n",
    "    columns_features = [\n",
    "        \"afvi_median\",\"afvi_median_dry\",\"afvi_median_wet\",\"avi_median\",\"avi_median_dry\",\"avi_median_wet\",\n",
    "        \"awei_median\",\"awei_median_dry\",\"awei_median_wet\",\"blue_median\",\"blue_median_dry\",\n",
    "        \"blue_median_wet\",\"blue_stdDev\",\"brba_median\",\"brba_median_dry\",\"brba_median_wet\",\n",
    "        \"brightness_median\",\"brightness_median_dry\",\"brightness_median_wet\",\"bsi_median\",\"bsi_median_1\",\n",
    "        \"bsi_median_2\",\"cvi_median\",\"cvi_median_dry\",\"cvi_median_wet\",\n",
    "        \"dswi5_median\",\"dswi5_median_dry\",\"dswi5_median_wet\",\"evi_median\",\"evi_median_dry\",\n",
    "        \"evi_median_wet\",\"gcvi_median\",\"gcvi_median_dry\",\"gcvi_median_wet\",\"gemi_median\",\n",
    "        \"gemi_median_dry\",\"gemi_median_wet\",\"gli_median\",\"gli_median_dry\",\"gli_median_wet\",\n",
    "        \"green_median\",\"green_median_dry\",\"green_median_wet\",\"green_stdDev\",\"gvmi_median\",\n",
    "        \"gvmi_median_dry\",\"gvmi_median_wet\",\"hillshade\",\"iia_median\",\"iia_median_dry\",\n",
    "        \"iia_median_wet\",\"lswi_median\",\"lswi_median_dry\",\"lswi_median_wet\",\"mbi_median\",\n",
    "        \"mbi_median_dry\",\"mbi_median_wet\",\"nddi_median\",\"nddi_median_dry\",\"nddi_median_wet\",\n",
    "        \"ndvi_median\",\"ndvi_median_dry\",\"ndvi_median_wet\",\"ndwi_median\",\"ndwi_median_dry\",\n",
    "        \"ndwi_median_wet\",\"nir_median\",\"nir_median_contrast\",\"nir_median_dry\",\"nir_median_dry_contrast\",\n",
    "        \"nir_median_wet\",\"nir_stdDev\",\"osavi_median\",\"osavi_median_dry\",\"osavi_median_wet\",\n",
    "        \"ratio_median\",\"ratio_median_dry\",\"ratio_median_wet\",\"red_median\",\"red_median_contrast\",\n",
    "        \"red_median_dry\",\"red_median_dry_contrast\",\"red_median_wet\",\"red_stdDev\",\"ri_median\",\n",
    "        \"ri_median_dry\",\"ri_median_wet\",\"rvi_median\",\"rvi_median_1\",\"rvi_median_wet\",\n",
    "        \"shape_median\",\"shape_median_dry\",\"shape_median_wet\",\"solpe\",\"swir1_median\",\n",
    "        \"swir1_median_dry\",\"swir1_median_wet\",\"swir1_stdDev\",\"swir2_median\",\"swir2_median_dry\",\n",
    "        \"swir2_median_wet\",\"swir2_stdDev\",\"ui_median\",\"ui_median_dry\",\"ui_median_wet\",\n",
    "        \"wetness_median\",\"wetness_median_dry\",\"wetness_median_wet\",\n",
    "    ]\n",
    "    classe = \"class\"\n",
    "\n",
    "    def __init__(self, Ns_estimators, learning_rates, path_features):\n",
    "        self.dfROIs = None\n",
    "        self.dfCC = None\n",
    "        self.yearAct = None\n",
    "        self.lstClass = None\n",
    "        self.lst_N_estimators = Ns_estimators\n",
    "        self.lst_learning_rate = learning_rates\n",
    "        self.path_features = path_features\n",
    "        self.betterPmtrosSet = 0\n",
    "        self.dictpmtGTB = {}\n",
    "        count = 0\n",
    "        for ne in self.lst_N_estimators:\n",
    "            for lr in self.lst_learning_rate:\n",
    "                self.dictpmtGTB[str(count)] = [ne, lr]\n",
    "                print(f\"# {count + 1} mudando n_estimators= {ne} & learning_rate= {lr}\")\n",
    "                count += 1\n",
    "\n",
    "    def get_data(self, myDF, nYear):\n",
    "        self.dfROIs = myDF\n",
    "        self.yearAct = nYear\n",
    "        self.lstClass = self.dfROIs[self.classe].unique().tolist()\n",
    "        self.buildingPercentsofClass()\n",
    "\n",
    "    def get_class_withSmallsize(self, dFrames, lstSearch):\n",
    "        classeMin = []\n",
    "        for cclass in lstSearch:\n",
    "\n",
    "            nsize = dFrames[dFrames[self.classe] == cclass].shape[0]\n",
    "            print(f\" classe {cclass} == > size = {nsize}\")\n",
    "            if nsize < 4:\n",
    "                classeMin.append(cclass)\n",
    "        return classeMin\n",
    "\n",
    "    def split_dataFrame(self, dFrame):\n",
    "        # split data into inputs (X) and outputs (y)\n",
    "        dFrame4 = dFrame[dFrame[self.classe] == 4]\n",
    "        dFrameO = dFrame[dFrame[self.classe] != 4]\n",
    "        # lstClasses  = [kk for kk in self.lstClass if kk != 4]\n",
    "        addFeatext = False\n",
    "\n",
    "        maximoROIs = self.dfCC[self.dfCC['class'] != 4]['count'].max()\n",
    "        maximoROIs += 150\n",
    "        newlstDF = []\n",
    "        print(\"size dFrame4 \", dFrame4.shape, \" and the next class maximum is \", maximoROIs)\n",
    "        # sampled the N samples fro dataframe stratified\n",
    "        tmpDF = dFrame4.sample(n= int(maximoROIs), random_state= np.random.seed(int(maximoROIs/ 2)), replace= True)  #\n",
    "        concDF  = pd.concat([tmpDF, dFrameO], ignore_index=True) #\n",
    "        print(\"temos {} filas \".format(concDF.shape))\n",
    "        # concDF.head()\n",
    "        lstCCg1 = [3,4,15,18]\n",
    "        lstCCg2 = [12,21,22,33]\n",
    "        print(\" ====> analisando size of class smaller \")\n",
    "        lstclassMinM = self.get_class_withSmallsize(dFrame, lstCCg2)\n",
    "\n",
    "        if len(lstclassMinM)  > 0:\n",
    "            for ccm in lstclassMinM:\n",
    "                print(\" --- will be remove class ---\", ccm)\n",
    "                lstCCg2.remove(ccm)\n",
    "            addFeatext = True\n",
    "\n",
    "\n",
    "        dFrameg1 = concDF[concDF[self.classe].isin(lstCCg1)]\n",
    "        dFrameg2 = concDF[concDF[self.classe].isin(lstCCg2)]\n",
    "\n",
    "        # print(f\" adding {int(propCC * maximoROIs)} samples from class [{cclass}]\")\n",
    "        # X = dataFrame[self.columns_features[:]]\n",
    "        # y = dataFrame[self.classe]\n",
    "        X_traing1, X_testg1, y_traing1, y_testg1 = train_test_split(\n",
    "                            dFrameg1[self.columns_features[:]], dFrameg1[self.classe],\n",
    "                            train_size=0.05,\n",
    "                            random_state=1,\n",
    "                            shuffle=True,\n",
    "                            stratify = dFrameg1[self.classe]\n",
    "                        )\n",
    "        print(f\"colected Xtrain {X_traing1.shape[0]} | Xtest {X_testg1.shape[0]} | \" +\n",
    "                                f\"ytrain {y_traing1.shape[0]} | ytest {y_testg1.shape[0]}\")\n",
    "        X_traing2, X_testg2, y_traing2, y_testg2 = train_test_split(\n",
    "                            dFrameg2[self.columns_features[:]], dFrameg2[self.classe],\n",
    "                            train_size=0.9,\n",
    "                            random_state=1,\n",
    "                            shuffle=True,\n",
    "                            stratify = dFrameg2[self.classe]\n",
    "                        )\n",
    "        print(f\"colected Xtrain {X_traing2.shape[0]} | Xtest {X_testg2.shape[0]} | \" +\n",
    "                                f\"ytrain {y_traing2.shape[0]} | ytest {y_testg2.shape[0]}\")\n",
    "\n",
    "\n",
    "        self.X_train = pd.concat([X_traing1, X_traing2], ignore_index=True)\n",
    "        self.X_test = pd.concat([X_testg1, X_testg2], ignore_index=True)\n",
    "        self.y_train = pd.concat([y_traing1, y_traing2], ignore_index=True)\n",
    "        self.y_test = pd.concat([y_testg1, y_testg2], ignore_index=True)\n",
    "\n",
    "        print(f\" ==== know we have {self.X_train.shape} to train ==== \")\n",
    "        print(self.y_train.value_counts(normalize= True), self.y_train.value_counts())\n",
    "\n",
    "\n",
    "    def buildingPercentsofClass(self):\n",
    "        self.dfCC = self.dfROIs['class'].value_counts()\n",
    "        self.dfCC = self.dfCC.reset_index()\n",
    "        # get total number of classes\n",
    "        total = np.sum(self.dfCC['count'].tolist())\n",
    "        print(f\"total = {total}\")\n",
    "        self.dfCC['percent'] = round((self.dfCC['count'] * 100)/ total, 2)\n",
    "        self.dfCC['Years'] = np.ones(self.dfCC.shape[0]).astype(int) * self.yearAct\n",
    "        # print(dfCC)\n",
    "        print(f\" == the CLASS of rois distribuided in {self.yearAct} are == \\n \", self.dfCC)\n",
    "\n",
    "    def processingMultiplesModels(self):\n",
    "        # split the dataframe in stratify samples by class and balance class 4\n",
    "        self.split_dataFrame(self.dfROIs)\n",
    "        maximAcc = 0.0\n",
    "        # get the models to evaluate\n",
    "        models = self.get_models()\n",
    "        # evaluate the models and store results\n",
    "        results, names = list(), list()\n",
    "        start = time.time()\n",
    "        count = 1\n",
    "        for name, model in models.items():\n",
    "            print(f\"#{count}/{len(models.items())} processing model {name}\")\n",
    "            scores = self.evaluate_model(model, self.X_train[self.columns_features[:]], self.y_train)\n",
    "            results.append(scores)\n",
    "            names.append(name)\n",
    "            print('  >%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n",
    "            if maximAcc < np.mean(scores):\n",
    "                self.betterPmtrosSet = count - 1\n",
    "                maximAcc = np.mean(scores)\n",
    "            count += 1\n",
    "        # plot model performance for comparison\n",
    "        # plt.boxplot(results, labels=names, showmeans=True)\n",
    "        # plt.show()\n",
    "        end = time.time()\n",
    "        tiempo = end - start\n",
    "        if tiempo < 60:\n",
    "            print(f\"model trained in {tiempo} seconds\")\n",
    "        else:\n",
    "            print(f\"model trained in {tiempo/60} minutos\")\n",
    "\n",
    "\n",
    "    # get a list of models to evaluate\n",
    "    def get_models(self):\n",
    "        min_features_to_select = 7\n",
    "        models = dict()\n",
    "\n",
    "        # criando pipeline do modelos gradiente Boosting com varios paramentros\n",
    "        cv = StratifiedKFold(3)\n",
    "        for cc in range(len(self.dictpmtGTB.keys())):\n",
    "            GTBmodel = GradientBoostingClassifier(\n",
    "                            n_estimators= self.dictpmtGTB[str(cc)][0],\n",
    "                            learning_rate= self.dictpmtGTB[str(cc)][1],\n",
    "                            max_features= 7\n",
    "                        )\n",
    "            rfe = RFECV(\n",
    "                    estimator=GTBmodel,\n",
    "                    step=1,\n",
    "                    cv=cv,\n",
    "                    scoring=\"accuracy\",\n",
    "                    min_features_to_select=min_features_to_select,\n",
    "                    n_jobs= -1,\n",
    "                )\n",
    "\n",
    "            models[str(cc)] = Pipeline(steps=[('s', rfe),('m', GTBmodel)])\n",
    "        return models\n",
    "\n",
    "    # evaluate a give model using cross-validation\n",
    "    def evaluate_model(self, model, X, y):\n",
    "        cv = StratifiedKFold(3)\n",
    "        scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv,  error_score='raise') # n_jobs=-1,\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def get_better_featuresSet(self, fixarNumbFeat, numbMin, regg, yyear):\n",
    "\n",
    "        # Create a Path object\n",
    "        path_name_file = self.path_features + f'/featuresSelectS2_{regg}_{yyear}.csv'\n",
    "        file_path = Path(path_name_file)\n",
    "\n",
    "        # Check if the file exists\n",
    "        if file_path.exists():\n",
    "            print(\" ******* list of features selected was saved ********\")\n",
    "        else:\n",
    "            self.split_dataFrame(self.dfROIs)\n",
    "            GTBmodel = GradientBoostingClassifier(\n",
    "                            n_estimators= self.dictpmtGTB[str(self.betterPmtrosSet)][0],\n",
    "                            learning_rate= self.dictpmtGTB[str(self.betterPmtrosSet)][1],\n",
    "                            max_features= 7,\n",
    "                            random_state=42\n",
    "                        )\n",
    "\n",
    "            start = time.time()\n",
    "            # Minimum number of features to consider\n",
    "            min_features_to_select =  7\n",
    "            cv = StratifiedKFold(3)\n",
    "            rfecv = RFECV(\n",
    "                estimator= GTBmodel,\n",
    "                step= 1,\n",
    "                cv= cv,\n",
    "                scoring= \"accuracy\",\n",
    "                min_features_to_select= min_features_to_select,\n",
    "                n_jobs= -1,\n",
    "            )\n",
    "            rfecv.fit(self.X_train[self.columns_features], self.y_train)\n",
    "            print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "            \n",
    "            end = time.time()\n",
    "            tiempo = end - start\n",
    "            if tiempo < 60:\n",
    "                print(f\"model trained in {tiempo} seconds\")\n",
    "            else:\n",
    "                print(f\"model trained in {tiempo/60} minutos\")\n",
    "\n",
    "            ## valores ótimos aparecem com valor 1 no ranking\n",
    "            lst_ranking = [(kk, cc) for cc, kk in enumerate(rfecv.ranking_) if kk < 2]\n",
    "            print(\"quantos features otimos \", len(lst_ranking))\n",
    "            numbNotOti = numbMin\n",
    "            \n",
    "            if fixarNumbFeat and numbMin > len(lst_ranking):\n",
    "                numbNotOti = numbMin - len(lst_ranking)\n",
    "                print(f\"Addicionando << {numbNotOti} >> features a mais não ótimas \")\n",
    "                lst_ranking_tmp = [(kk, cc) for cc, kk in enumerate(rfecv.ranking_) if kk < numbNotOti]\n",
    "                lst_ranking += lst_ranking_tmp\n",
    "            print(\"quantos features otimos \", len(lst_ranking_tmp))                \n",
    "\n",
    "            lstFeatSelect = []\n",
    "            ccount = 1\n",
    "            for kk, cc in lst_ranking:\n",
    "                print(f\"# {ccount} ranking {kk} | pos {cc} >> feature >> {self.columns_features[cc]}\")\n",
    "                lstFeatSelect.append(self.columns_features[cc])\n",
    "                ccount += 1\n",
    "\n",
    "            dict_result= {\n",
    "                'ranking': lst_ranking,\n",
    "                'features': lstFeatSelect\n",
    "            }\n",
    "            dfresult = pd.DataFrame.from_dict(dict_result)\n",
    "            os.system(f\"mkdir {path_name_file}\") \n",
    "            os.chmod(path_name_file, 777)                   \n",
    "            dfresult.to_csv(path_name_file, index= False)\n",
    "\n",
    "            print(f\"tabela {path_name_file.split('/')[-1]} salva na pasta de output folder\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "yW0vlol_H1VQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yW0vlol_H1VQ",
    "outputId": "890db644-5879-41a2-d0cf-7b843f85ffc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " year  [2024, 2023, 2022, 2021, 2020, 2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2007, 2006, 2005, 2004, 2003, 2002, 2001, 2000, 1999, 1998, 1997, 1996, 1995, 1994, 1993, 1992, 1991, 1990, 1989, 1988, 1987, 1986]\n"
     ]
    }
   ],
   "source": [
    "lstYear = list(range(2024, 1985, -1))\n",
    "print(\" year \", lstYear);\n",
    "# sys.exit()\n",
    "# fixar o número de variaveis\n",
    "fixarNFeat = True\n",
    "# número máximo de variaveis para o modelo das 144\n",
    "numMin = 70\n",
    "yyear = 2023\n",
    "lstEstimadors = [15, 20, 30, 40, 50, 60]\n",
    "lstLearnRate = [0.001, 0.005, 0.01, 0.1]\n",
    "melhorModelo = 0\n",
    "dictModel = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0AXgPLMGfarP",
   "metadata": {
    "id": "0AXgPLMGfarP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0UGNM1nFhwuT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UGNM1nFhwuT",
    "outputId": "b522391d-fea7-4a16-a161-eb4913da41ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1 mudando n_estimators= 15 & learning_rate= 0.001\n",
      "# 2 mudando n_estimators= 15 & learning_rate= 0.005\n",
      "# 3 mudando n_estimators= 15 & learning_rate= 0.01\n",
      "# 4 mudando n_estimators= 15 & learning_rate= 0.1\n",
      "# 5 mudando n_estimators= 20 & learning_rate= 0.001\n",
      "# 6 mudando n_estimators= 20 & learning_rate= 0.005\n",
      "# 7 mudando n_estimators= 20 & learning_rate= 0.01\n",
      "# 8 mudando n_estimators= 20 & learning_rate= 0.1\n",
      "# 9 mudando n_estimators= 30 & learning_rate= 0.001\n",
      "# 10 mudando n_estimators= 30 & learning_rate= 0.005\n",
      "# 11 mudando n_estimators= 30 & learning_rate= 0.01\n",
      "# 12 mudando n_estimators= 30 & learning_rate= 0.1\n",
      "# 13 mudando n_estimators= 40 & learning_rate= 0.001\n",
      "# 14 mudando n_estimators= 40 & learning_rate= 0.005\n",
      "# 15 mudando n_estimators= 40 & learning_rate= 0.01\n",
      "# 16 mudando n_estimators= 40 & learning_rate= 0.1\n",
      "# 17 mudando n_estimators= 50 & learning_rate= 0.001\n",
      "# 18 mudando n_estimators= 50 & learning_rate= 0.005\n",
      "# 19 mudando n_estimators= 50 & learning_rate= 0.01\n",
      "# 20 mudando n_estimators= 50 & learning_rate= 0.1\n",
      "# 21 mudando n_estimators= 60 & learning_rate= 0.001\n",
      "# 22 mudando n_estimators= 60 & learning_rate= 0.005\n",
      "# 23 mudando n_estimators= 60 & learning_rate= 0.01\n",
      "# 24 mudando n_estimators= 60 & learning_rate= 0.1\n",
      " os arquivos gerados vão se salvos aqui \n",
      " >>  /home/superuser/Dados/mapbiomas/dadosCol10/fileFeatSelect\n"
     ]
    }
   ],
   "source": [
    "# instanciar classe de processamento dos Features\n",
    "procFeatures_byYears = processin_features_byYears(lstEstimadors, lstLearnRate, pathFeaturesBase)\n",
    "print(\" os arquivos gerados vão se salvos aqui \\n >> \", procFeatures_byYears.path_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "i5tqkc1mh8mn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i5tqkc1mh8mn",
    "outputId": "6edca6a6-d398-4d1b-87e0-2e7dd4a572b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate dictModel => {}\n",
      "# 1 mudando n_estimators= 15 & learning_rate= 0.001\n",
      "# 2 mudando n_estimators= 15 & learning_rate= 0.005\n",
      "# 3 mudando n_estimators= 15 & learning_rate= 0.01\n",
      "# 4 mudando n_estimators= 15 & learning_rate= 0.1\n",
      "# 5 mudando n_estimators= 20 & learning_rate= 0.001\n",
      "# 6 mudando n_estimators= 20 & learning_rate= 0.005\n",
      "# 7 mudando n_estimators= 20 & learning_rate= 0.01\n",
      "# 8 mudando n_estimators= 20 & learning_rate= 0.1\n",
      "# 9 mudando n_estimators= 30 & learning_rate= 0.001\n",
      "# 10 mudando n_estimators= 30 & learning_rate= 0.005\n",
      "# 11 mudando n_estimators= 30 & learning_rate= 0.01\n",
      "# 12 mudando n_estimators= 30 & learning_rate= 0.1\n",
      "# 13 mudando n_estimators= 40 & learning_rate= 0.001\n",
      "# 14 mudando n_estimators= 40 & learning_rate= 0.005\n",
      "# 15 mudando n_estimators= 40 & learning_rate= 0.01\n",
      "# 16 mudando n_estimators= 40 & learning_rate= 0.1\n",
      "# 17 mudando n_estimators= 50 & learning_rate= 0.001\n",
      "# 18 mudando n_estimators= 50 & learning_rate= 0.005\n",
      "# 19 mudando n_estimators= 50 & learning_rate= 0.01\n",
      "# 20 mudando n_estimators= 50 & learning_rate= 0.1\n",
      "# 21 mudando n_estimators= 60 & learning_rate= 0.001\n",
      "# 22 mudando n_estimators= 60 & learning_rate= 0.005\n",
      "# 23 mudando n_estimators= 60 & learning_rate= 0.01\n",
      "# 24 mudando n_estimators= 60 & learning_rate= 0.1\n"
     ]
    }
   ],
   "source": [
    "dictModelsS = {}\n",
    "pathModelJson = '/home/superuser/Dados/mapbiomas/dadosCol10/dictBetterModelpmtCol10v1.json'\n",
    "try:\n",
    "    with open(pathModelJson, 'r') as fh:\n",
    "        dictModelsS = json.load(fh)\n",
    "    print(\"loaded dictModel => \", dictModelsS)\n",
    "    print(f\"with {len(dictModelsS.keys())} register\")\n",
    "except:\n",
    "    print(\"generate dictModel => {}\")\n",
    "\n",
    "count = 0\n",
    "for ne in lstEstimadors:\n",
    "    for lr in lstLearnRate:\n",
    "        dictModelsS[str(count)] = [ne, lr]\n",
    "        print(f\"# {count + 1} mudando n_estimators= {ne} & learning_rate= {lr}\")\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "gjCdWlyRoP3K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjCdWlyRoP3K",
    "outputId": "98590d54-7809-4df0-a8ac-ef8812e2a39f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': [15, 0.001],\n",
       " '1': [15, 0.005],\n",
       " '2': [15, 0.01],\n",
       " '3': [15, 0.1],\n",
       " '4': [20, 0.001],\n",
       " '5': [20, 0.005],\n",
       " '6': [20, 0.01],\n",
       " '7': [20, 0.1],\n",
       " '8': [30, 0.001],\n",
       " '9': [30, 0.005],\n",
       " '10': [30, 0.01],\n",
       " '11': [30, 0.1],\n",
       " '12': [40, 0.001],\n",
       " '13': [40, 0.005],\n",
       " '14': [40, 0.01],\n",
       " '15': [40, 0.1],\n",
       " '16': [50, 0.001],\n",
       " '17': [50, 0.005],\n",
       " '18': [50, 0.01],\n",
       " '19': [50, 0.1],\n",
       " '20': [60, 0.001],\n",
       " '21': [60, 0.005],\n",
       " '22': [60, 0.01],\n",
       " '23': [60, 0.1]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictModelsS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "KwKMr4zFH0dM",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwKMr4zFH0dM",
    "outputId": "3837b2e1-32af-4292-eac6-af3d394b2352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ==> /home/superuser/Dados/mapbiomas/dadosCol10/ROIsv2/ROIsv2/7746.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/superuser/Dados/mapbiomas/dadosCol10/dictBetterModelpmtCol10v1_7746.json’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  columns =  Index(['GRID_ID', 'afvi_median', 'afvi_median_dry', 'afvi_median_wet',\n",
      "       'avi_median', 'avi_median_dry', 'avi_median_wet', 'awei_median',\n",
      "       'awei_median_dry', 'awei_median_wet',\n",
      "       ...\n",
      "       'swir2_median_dry', 'swir2_median_wet', 'swir2_stdDev', 'ui_median',\n",
      "       'ui_median_dry', 'ui_median_wet', 'wetness_median',\n",
      "       'wetness_median_dry', 'wetness_median_wet', 'year'],\n",
      "      dtype='object', length=111)\n",
      " == Know how many rois have in the table == \n",
      "  year\n",
      "1999    2170\n",
      "1994    2135\n",
      "2017    2135\n",
      "2019    2100\n",
      "2018    2100\n",
      "1998    2100\n",
      "2020    2065\n",
      "1996    2065\n",
      "2010    2065\n",
      "2013    2065\n",
      "2009    2065\n",
      "2023    2030\n",
      "2012    2030\n",
      "1997    2030\n",
      "2000    2030\n",
      "2024    2030\n",
      "2008    2030\n",
      "1992    2030\n",
      "2021    2030\n",
      "2022    2030\n",
      "1995    1995\n",
      "1993    1995\n",
      "1986    1960\n",
      "1985    1960\n",
      "2007    1925\n",
      "2006    1925\n",
      "2011    1925\n",
      "2014    1925\n",
      "2004    1925\n",
      "2005    1925\n",
      "2003    1925\n",
      "2016    1890\n",
      "2015    1855\n",
      "2001    1820\n",
      "1990    1820\n",
      "1991    1820\n",
      "2002    1785\n",
      "1987    1785\n",
      "1989    1715\n",
      "1988    1575\n",
      "Name: count, dtype: int64\n",
      "total = 2030\n",
      " == the CLASS of rois distribuided in 2023 are == \n",
      "     class  count  percent  Years\n",
      "0      4   1015    50.00   2023\n",
      "1     15    420    20.69   2023\n",
      "2      3    315    15.52   2023\n",
      "3     12    140     6.90   2023\n",
      "4     21    105     5.17   2023\n",
      "5     22     35     1.72   2023\n",
      " basin activate 7746 and the other Regions (2030, 111) processadas \n",
      " ==>  ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '7746']\n",
      "selecionou model feito  {'better_pmtSet': 11, 'n_estimators': 30, 'learning_rate': 0.1}\n",
      "total = 2030\n",
      " == the CLASS of rois distribuided in 2024 are == \n",
      "     class  count  percent  Years\n",
      "0      4   1015    50.00   2024\n",
      "1     15    420    20.69   2024\n",
      "2      3    315    15.52   2024\n",
      "3     12    140     6.90   2024\n",
      "4     21    105     5.17   2024\n",
      "5     22     35     1.72   2024\n",
      "size dFrame4  (1015, 111)  and the next class maximum is  570\n",
      "temos (1585, 111) filas \n",
      " ====> analisando size of class smaller \n",
      " classe 12 == > size = 140\n",
      " classe 21 == > size = 105\n",
      " classe 22 == > size = 35\n",
      " classe 33 == > size = 0\n",
      " --- will be remove class --- 33\n",
      "colected Xtrain 65 | Xtest 1240 | ytrain 65 | ytest 1240\n",
      "colected Xtrain 252 | Xtest 28 | ytrain 252 | ytest 28\n",
      " ==== know we have (317, 108) to train ==== \n",
      "class\n",
      "12    0.397476\n",
      "21    0.299685\n",
      "22    0.097792\n",
      "4     0.088328\n",
      "15    0.066246\n",
      "3     0.050473\n",
      "Name: proportion, dtype: float64 class\n",
      "12    126\n",
      "21     95\n",
      "22     31\n",
      "4      28\n",
      "15     21\n",
      "3      16\n",
      "Name: count, dtype: int64\n",
      "Optimal number of features: 35\n",
      "model trained in 23.006421089172363 seconds\n",
      "quantos features otimos  35\n",
      "Addicionando << 35 >> features a mais não ótimas \n",
      "quantos features otimos  68\n",
      "# 1 ranking 1 | pos 1 >> feature >> afvi_median_dry\n",
      "# 2 ranking 1 | pos 4 >> feature >> avi_median_dry\n",
      "# 3 ranking 1 | pos 10 >> feature >> blue_median_dry\n",
      "# 4 ranking 1 | pos 11 >> feature >> blue_median_wet\n",
      "# 5 ranking 1 | pos 14 >> feature >> brba_median_dry\n",
      "# 6 ranking 1 | pos 15 >> feature >> brba_median_wet\n",
      "# 7 ranking 1 | pos 17 >> feature >> brightness_median_dry\n",
      "# 8 ranking 1 | pos 24 >> feature >> cvi_median_wet\n",
      "# 9 ranking 1 | pos 27 >> feature >> dswi5_median_wet\n",
      "# 10 ranking 1 | pos 28 >> feature >> evi_median\n",
      "# 11 ranking 1 | pos 29 >> feature >> evi_median_dry\n",
      "# 12 ranking 1 | pos 30 >> feature >> evi_median_wet\n",
      "# 13 ranking 1 | pos 32 >> feature >> gcvi_median_dry\n",
      "# 14 ranking 1 | pos 35 >> feature >> gemi_median_dry\n",
      "# 15 ranking 1 | pos 39 >> feature >> gli_median_wet\n",
      "# 16 ranking 1 | pos 49 >> feature >> iia_median_dry\n",
      "# 17 ranking 1 | pos 61 >> feature >> ndvi_median_dry\n",
      "# 18 ranking 1 | pos 64 >> feature >> ndwi_median_dry\n",
      "# 19 ranking 1 | pos 67 >> feature >> nir_median_contrast\n",
      "# 20 ranking 1 | pos 68 >> feature >> nir_median_dry\n",
      "# 21 ranking 1 | pos 69 >> feature >> nir_median_dry_contrast\n",
      "# 22 ranking 1 | pos 72 >> feature >> osavi_median\n",
      "# 23 ranking 1 | pos 73 >> feature >> osavi_median_dry\n",
      "# 24 ranking 1 | pos 79 >> feature >> red_median_contrast\n",
      "# 25 ranking 1 | pos 80 >> feature >> red_median_dry\n",
      "# 26 ranking 1 | pos 81 >> feature >> red_median_dry_contrast\n",
      "# 27 ranking 1 | pos 82 >> feature >> red_median_wet\n",
      "# 28 ranking 1 | pos 83 >> feature >> red_stdDev\n",
      "# 29 ranking 1 | pos 88 >> feature >> rvi_median_1\n",
      "# 30 ranking 1 | pos 90 >> feature >> shape_median\n",
      "# 31 ranking 1 | pos 92 >> feature >> shape_median_wet\n",
      "# 32 ranking 1 | pos 96 >> feature >> swir1_median_wet\n",
      "# 33 ranking 1 | pos 100 >> feature >> swir2_median_wet\n",
      "# 34 ranking 1 | pos 105 >> feature >> wetness_median\n",
      "# 35 ranking 1 | pos 106 >> feature >> wetness_median_dry\n",
      "# 36 ranking 1 | pos 1 >> feature >> afvi_median_dry\n",
      "# 37 ranking 25 | pos 2 >> feature >> afvi_median_wet\n",
      "# 38 ranking 3 | pos 3 >> feature >> avi_median\n",
      "# 39 ranking 1 | pos 4 >> feature >> avi_median_dry\n",
      "# 40 ranking 31 | pos 5 >> feature >> avi_median_wet\n",
      "# 41 ranking 32 | pos 8 >> feature >> awei_median_wet\n",
      "# 42 ranking 1 | pos 10 >> feature >> blue_median_dry\n",
      "# 43 ranking 1 | pos 11 >> feature >> blue_median_wet\n",
      "# 44 ranking 1 | pos 14 >> feature >> brba_median_dry\n",
      "# 45 ranking 1 | pos 15 >> feature >> brba_median_wet\n",
      "# 46 ranking 1 | pos 17 >> feature >> brightness_median_dry\n",
      "# 47 ranking 29 | pos 18 >> feature >> brightness_median_wet\n",
      "# 48 ranking 20 | pos 21 >> feature >> bsi_median_2\n",
      "# 49 ranking 10 | pos 22 >> feature >> cvi_median\n",
      "# 50 ranking 34 | pos 23 >> feature >> cvi_median_dry\n",
      "# 51 ranking 1 | pos 24 >> feature >> cvi_median_wet\n",
      "# 52 ranking 30 | pos 26 >> feature >> dswi5_median_dry\n",
      "# 53 ranking 1 | pos 27 >> feature >> dswi5_median_wet\n",
      "# 54 ranking 1 | pos 28 >> feature >> evi_median\n",
      "# 55 ranking 1 | pos 29 >> feature >> evi_median_dry\n",
      "# 56 ranking 1 | pos 30 >> feature >> evi_median_wet\n",
      "# 57 ranking 1 | pos 32 >> feature >> gcvi_median_dry\n",
      "# 58 ranking 12 | pos 33 >> feature >> gcvi_median_wet\n",
      "# 59 ranking 16 | pos 34 >> feature >> gemi_median\n",
      "# 60 ranking 1 | pos 35 >> feature >> gemi_median_dry\n",
      "# 61 ranking 14 | pos 37 >> feature >> gli_median\n",
      "# 62 ranking 23 | pos 38 >> feature >> gli_median_dry\n",
      "# 63 ranking 1 | pos 39 >> feature >> gli_median_wet\n",
      "# 64 ranking 28 | pos 40 >> feature >> green_median\n",
      "# 65 ranking 21 | pos 43 >> feature >> green_stdDev\n",
      "# 66 ranking 5 | pos 45 >> feature >> gvmi_median_dry\n",
      "# 67 ranking 1 | pos 49 >> feature >> iia_median_dry\n",
      "# 68 ranking 15 | pos 52 >> feature >> lswi_median_dry\n",
      "# 69 ranking 26 | pos 58 >> feature >> nddi_median_dry\n",
      "# 70 ranking 22 | pos 59 >> feature >> nddi_median_wet\n",
      "# 71 ranking 1 | pos 61 >> feature >> ndvi_median_dry\n",
      "# 72 ranking 1 | pos 64 >> feature >> ndwi_median_dry\n",
      "# 73 ranking 18 | pos 65 >> feature >> ndwi_median_wet\n",
      "# 74 ranking 7 | pos 66 >> feature >> nir_median\n",
      "# 75 ranking 1 | pos 67 >> feature >> nir_median_contrast\n",
      "# 76 ranking 1 | pos 68 >> feature >> nir_median_dry\n",
      "# 77 ranking 1 | pos 69 >> feature >> nir_median_dry_contrast\n",
      "# 78 ranking 17 | pos 71 >> feature >> nir_stdDev\n",
      "# 79 ranking 1 | pos 72 >> feature >> osavi_median\n",
      "# 80 ranking 1 | pos 73 >> feature >> osavi_median_dry\n",
      "# 81 ranking 8 | pos 74 >> feature >> osavi_median_wet\n",
      "# 82 ranking 11 | pos 76 >> feature >> ratio_median_dry\n",
      "# 83 ranking 13 | pos 77 >> feature >> ratio_median_wet\n",
      "# 84 ranking 1 | pos 79 >> feature >> red_median_contrast\n",
      "# 85 ranking 1 | pos 80 >> feature >> red_median_dry\n",
      "# 86 ranking 1 | pos 81 >> feature >> red_median_dry_contrast\n",
      "# 87 ranking 1 | pos 82 >> feature >> red_median_wet\n",
      "# 88 ranking 1 | pos 83 >> feature >> red_stdDev\n",
      "# 89 ranking 6 | pos 85 >> feature >> ri_median_dry\n",
      "# 90 ranking 2 | pos 86 >> feature >> ri_median_wet\n",
      "# 91 ranking 1 | pos 88 >> feature >> rvi_median_1\n",
      "# 92 ranking 27 | pos 89 >> feature >> rvi_median_wet\n",
      "# 93 ranking 1 | pos 90 >> feature >> shape_median\n",
      "# 94 ranking 1 | pos 92 >> feature >> shape_median_wet\n",
      "# 95 ranking 19 | pos 95 >> feature >> swir1_median_dry\n",
      "# 96 ranking 1 | pos 96 >> feature >> swir1_median_wet\n",
      "# 97 ranking 24 | pos 97 >> feature >> swir1_stdDev\n",
      "# 98 ranking 33 | pos 99 >> feature >> swir2_median_dry\n",
      "# 99 ranking 1 | pos 100 >> feature >> swir2_median_wet\n",
      "# 100 ranking 9 | pos 103 >> feature >> ui_median_dry\n",
      "# 101 ranking 1 | pos 105 >> feature >> wetness_median\n",
      "# 102 ranking 1 | pos 106 >> feature >> wetness_median_dry\n",
      "# 103 ranking 4 | pos 107 >> feature >> wetness_median_wet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/superuser/Dados/mapbiomas/dadosCol10/fileFeatSelect/featuresSelectS2_7746_2024.csv’: Permission denied\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/superuser/Dados/mapbiomas/dadosCol10/fileFeatSelect/featuresSelectS2_7746_2024.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m dftableYY \u001b[38;5;241m=\u001b[39m dftable[dftable[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m nyear]\n\u001b[1;32m     43\u001b[0m procFeatures_byYears\u001b[38;5;241m.\u001b[39mget_data(dftableYY, nyear)\n\u001b[0;32m---> 44\u001b[0m \u001b[43mprocFeatures_byYears\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_better_featuresSet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfixarNFeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumMin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbacia\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnyear\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 264\u001b[0m, in \u001b[0;36mprocessin_features_byYears.get_better_featuresSet\u001b[0;34m(self, fixarNumbFeat, numbMin, regg, yyear)\u001b[0m\n\u001b[1;32m    262\u001b[0m dfresult \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(dict_result)\n\u001b[1;32m    263\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmkdir \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_name_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m--> 264\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_name_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m777\u001b[39;49m\u001b[43m)\u001b[49m                   \n\u001b[1;32m    265\u001b[0m dfresult\u001b[38;5;241m.\u001b[39mto_csv(path_name_file, index\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtabela \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_name_file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m salva na pasta de output folder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/superuser/Dados/mapbiomas/dadosCol10/fileFeatSelect/featuresSelectS2_7746_2024.csv'"
     ]
    }
   ],
   "source": [
    "for cc, npath in enumerate(lstpathfiles[3:]):\n",
    "    df_bacia = {}    \n",
    "    print(\"Loading ==> \" + npath)\n",
    "    nbacia = npath.split(\"/\")[-1].replace('.csv', '')    \n",
    "    pathModelJson = f'/home/superuser/Dados/mapbiomas/dadosCol10/dictBetterModelpmtCol10v1_{nbacia}.json'\n",
    "    os.system(f\"mkdir {pathModelJson}\") \n",
    "    # os.system(f'chmod 777 {pathModelJson}')\n",
    "    os.chmod(pathModelJson, 777)\n",
    "    dftable = pd.read_csv(npath)\n",
    "    dftable = dftable.drop(['system:index','.geo'], axis=1)\n",
    "    print(\"  columns = \", dftable.columns)\n",
    "    print(f\" == Know how many rois have in the table == \\n \",\n",
    "                                dftable.year.value_counts())\n",
    "\n",
    "    dftableYY = dftable[dftable['year'] == yyear]\n",
    "    procFeatures_byYears.get_data(dftableYY, yyear)\n",
    "    lstKeysBa = [kk for kk in dictModelsS.keys()]\n",
    "    print(f\" basin activate {nbacia} and the other Regions {dftableYY.shape} processadas \\n ==> \", lstKeysBa)\n",
    "\n",
    "    if nbacia not in lstKeysBa:\n",
    "        procFeatures_byYears.processingMultiplesModels()\n",
    "        melhorModelo = procFeatures_byYears.betterPmtrosSet\n",
    "        dictModelsS[nbacia] = {\n",
    "            'better_pmtSet': melhorModelo,\n",
    "            'n_estimators': dictModelsS[str(melhorModelo)][0],\n",
    "            'learning_rate': dictModelsS[str(melhorModelo)][1]\n",
    "        }\n",
    "        # Convert and write JSON object to file\n",
    "        with open(pathModelJson, \"w\") as outfile:\n",
    "            os.system(f\"mkdir {pathModelJson}\") \n",
    "            # os.system(f'chmod 777 {pathModelJson}')\n",
    "            os.chmod(pathModelJson, 777)\n",
    "            # os.chmod(pathModelJson, 0o666)\n",
    "            json.dump(dictModelsS, outfile)\n",
    "\n",
    "    else:\n",
    "        df_bacia = dictModelsS[nbacia]\n",
    "        print(\"selecionou model feito \", df_bacia)\n",
    "        procFeatures_byYears.betterPmtrosSet = df_bacia['better_pmtSet']\n",
    "    # break\n",
    "    for nyear in lstYear:\n",
    "        dftableYY = dftable[dftable['year'] == nyear]\n",
    "        procFeatures_byYears.get_data(dftableYY, nyear)\n",
    "        procFeatures_byYears.get_better_featuresSet(fixarNFeat, numMin, nbacia, nyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UQpgeZEOHx0H",
   "metadata": {
    "id": "UQpgeZEOHx0H"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LRfSdjxyT9Pm",
   "metadata": {
    "id": "LRfSdjxyT9Pm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
